

---
- name: srSILO Update Pipeline - Full Automation
  hosts: srsilo
  gather_facts: yes
  
  pre_tasks:
    - name: Log pipeline start to journal
      shell: |
        systemd-cat -t srsilo-pipeline -p info <<EOF
        Pipeline started
        User: $(whoami)
        Fetch days: {{ srsilo_fetch_days }}
        Retention days: {{ srsilo_retention_days }}
        EOF
      become: yes
      become_user: "{{ srsilo_user }}"
      changed_when: false
  
  tasks:
    - name: Display pipeline configuration
      debug:
        msg:
          - "=== srSILO Update Pipeline ==="
          - "Iteration: 4b (Phases 1-2-3-4-5-6)"
          - "Base path: {{ srsilo_base_path }}"
          - "API: {{ srsilo_api_base_url }}"
          - "Fetch days: {{ srsilo_fetch_days }}"
          - "Fetch max reads: {{ srsilo_fetch_max_reads }}"
          - "Retention days: {{ srsilo_retention_days }}"
          - "Retention min keep: {{ srsilo_retention_min_keep }}"
    
    # ========================================================================
    # PHASE 1: Pre-flight Checks
    # ========================================================================
    
    - name: "PHASE 1: Setup prerequisites (user, directories, permissions)"
      include_role:
        name: srsilo
        tasks_from: prerequisites
      tags: [phase1, prerequisites]
    
    - name: "PHASE 1: Build Rust tools"
      include_role:
        name: srsilo
        tasks_from: build_tools
      tags: [phase1, build]
    
    - name: "PHASE 1: Deploy configuration files"
      include_role:
        name: srsilo
        tasks_from: deploy_configs
      tags: [phase1, config]
    
    - name: "PHASE 1: Pre-flight checks complete"
      debug:
        msg: "✓ Phase 1 complete - Prerequisites verified, tools built, and configs deployed"
    
    # ========================================================================
    # PHASE 2: Check for New Data
    # ========================================================================
    
    - name: "PHASE 2: Check for new data from LAPIS API"
      include_role:
        name: srsilo
        tasks_from: check_new_data
      tags: [phase2, check]
    
    # Exit gracefully if no new data
    - name: "PHASE 2: Exit if no new data found"
      block:
        - name: Display no new data message
          debug:
            msg:
              - "=== Pipeline Complete ==="
              - "No new data available - skipping remaining phases"
              - "This is normal and saves resources"
        
        - name: End playbook execution
          meta: end_play
      when: not srsilo_has_new_data
    
    - name: "PHASE 2: New data detected - pipeline will continue"
      debug:
        msg:
          - "=== New Data Detected ==="
          - "✓ Phase 2 complete - New data available"
          - "Proceeding to Phase 3: Cleanup old indexes"
      when: srsilo_has_new_data
    
    # ========================================================================
    # PHASE 3: Pre-processing Cleanup
    # ========================================================================
    
    - name: "PHASE 3: Run retention policy (cleanup old indexes)"
      include_role:
        name: srsilo
        tasks_from: cleanup_indexes
      tags: [phase3, cleanup]
      when: srsilo_has_new_data
    
    - name: "PHASE 3: Check for orphaned preprocessing indexes"
      block:
        - name: Check if preprocessing_in_progress marker exists
          stat:
            path: "{{ srsilo_data_output }}/.preprocessing_in_progress"
          register: preprocessing_marker
        
        - name: Read orphaned index timestamp
          slurp:
            path: "{{ srsilo_data_output }}/.preprocessing_in_progress"
          register: orphaned_timestamp
          when: preprocessing_marker.stat.exists
        
        - name: Delete orphaned preprocessing index
          file:
            path: "{{ srsilo_data_output }}/{{ orphaned_timestamp.content | b64decode | trim }}"
            state: absent
          become: yes
          when: 
            - preprocessing_marker.stat.exists
            - orphaned_timestamp.content is defined
          register: orphan_deletion
        
        - name: Remove preprocessing marker
          file:
            path: "{{ srsilo_data_output }}/.preprocessing_in_progress"
            state: absent
          become: yes
          when: preprocessing_marker.stat.exists
        
        - name: Display orphan cleanup result
          debug:
            msg: "✓ Cleaned up orphaned preprocessing from previous failed run"
          when: preprocessing_marker.stat.exists
      tags: [phase3, cleanup]
      when: srsilo_has_new_data
    
    - name: "PHASE 3: Clean old downloaded data"
      block:
        - name: Find old input files
          find:
            path: "{{ srsilo_data_input }}"
            patterns: "*.ndjson.zst"
            age: "1d"
          register: old_input_files
        
        - name: Remove old input files
          file:
            path: "{{ item.path }}"
            state: absent
          loop: "{{ old_input_files.files }}"
          become: yes
          when: old_input_files.matched > 0
        
        - name: Display input cleanup result
          debug:
            msg: "✓ Removed {{ old_input_files.matched }} old input file(s)"
          when: old_input_files.matched > 0
      tags: [phase3, cleanup]
      when: srsilo_has_new_data
    
    - name: "PHASE 3: Reset working directories"
      block:
        - name: Clean sorted chunks directory
          file:
            path: "{{ srsilo_data_sorted_chunks }}"
            state: absent
          become: yes
        
        - name: Recreate sorted chunks directory
          file:
            path: "{{ srsilo_data_sorted_chunks }}"
            state: directory
            owner: "{{ srsilo_user }}"
            group: "{{ srsilo_group }}"
            mode: '0755'
          become: yes
        
        - name: Clean tmp directory
          file:
            path: "{{ srsilo_data_tmp }}"
            state: absent
          become: yes
        
        - name: Recreate tmp directory
          file:
            path: "{{ srsilo_data_tmp }}"
            state: directory
            owner: "{{ srsilo_user }}"
            group: "{{ srsilo_group }}"
            mode: '0755'
          become: yes
      tags: [phase3, cleanup]
      when: srsilo_has_new_data
    
    - name: "PHASE 3: Cleanup complete"
      debug:
        msg:
          - "✓ Phase 3 complete - Cleanup finished"
          - "Proceeding to Phase 4: Fetch data"
      when: srsilo_has_new_data
    
    # ========================================================================
    # PHASE 4: Fetch Data
    # ========================================================================
    
    - name: "PHASE 4: Fetch data from LAPIS API"
      include_role:
        name: srsilo
        tasks_from: fetch_data
      vars:
        fetch_start_date: "{{ ansible_date_time.date }}"
      tags: [phase4, fetch]
      when: srsilo_has_new_data
    
    - name: "PHASE 4: Fetch complete"
      debug:
        msg:
          - "=== Phase 4 Complete ==="
          - "✓ Data fetched successfully"
          - "Proceeding to Phase 5: Prepare for processing"
      when: srsilo_has_new_data
    
    # ========================================================================
    # PHASE 5: Prepare for Processing
    # ========================================================================
    
    - name: "PHASE 5: Create preprocessing marker"
      block:
        - name: Get current timestamp
          set_fact:
            preprocessing_timestamp: "{{ ansible_date_time.epoch }}"
        
        - name: Create preprocessing_in_progress marker
          copy:
            content: "{{ preprocessing_timestamp }}"
            dest: "{{ srsilo_data_output }}/.preprocessing_in_progress"
            owner: "{{ srsilo_user }}"
            group: "{{ srsilo_group }}"
            mode: '0644'
          become: yes
        
        - name: Display marker created
          debug:
            msg: "✓ Created preprocessing marker: {{ preprocessing_timestamp }}"
      tags: [phase5, prepare]
      when: srsilo_has_new_data
    
    - name: "PHASE 5: Stop SILO API to free resources"
      include_role:
        name: srsilo
        tasks_from: manage_api
      vars:
        api_action: stop
      tags: [phase5, prepare, api]
      when: srsilo_has_new_data
    
    - name: "PHASE 5: Preparation complete"
      debug:
        msg:
          - "✓ Phase 5 complete - Ready for processing"
          - "API stopped, resources freed"
          - "Preprocessing marker created"
          - "Proceeding to Phase 6: Process data"
      when: srsilo_has_new_data
    
    # ========================================================================
    # PHASE 6 & 7: Process Data and Finalize (with Error Handling)
    # ========================================================================
    
    - name: "PHASES 6-7: Process data with automatic rollback on failure"
      block:
        - name: "PHASE 6: Sort and merge reads"
          include_role:
            name: srsilo
            tasks_from: sort_and_merge
          tags: [phase6, process]
        
        - name: "PHASE 6: Run SILO preprocessing"
          include_role:
            name: srsilo
            tasks_from: silo_preprocessing
          tags: [phase6, process, silo]
        
        - name: "PHASE 6: Processing complete"
          debug:
            msg:
              - "=== Phase 6 Complete ==="
              - "✓ Data processed successfully"
              - "✓ Sorted and merged reads"
              - "✓ SILO preprocessing finished"
        
        - name: "PHASE 7: Finalize processing and start API"
          include_role:
            name: srsilo
            tasks_from: finalize_processing
          tags: [phase7, finalize]
        
        - name: "PHASE 7: Pipeline complete"
          debug:
            msg:
              - "=== Full Pipeline Complete ==="
              - "✓ All phases executed successfully"
              - "✓ API running with latest index"
              - "✓ Ready for queries"
      
      rescue:
        - name: "RESCUE: Handle processing failure"
          debug:
            msg:
              - "=== Processing Failed - Initiating Rollback ==="
              - "Error: {{ ansible_failed_result.msg | default('Unknown error') }}"
        
        - name: "RESCUE: Run cleanup and rollback"
          include_role:
            name: srsilo
            tasks_from: finalize_processing
          vars:
            force_rollback: true
          tags: [rescue, rollback]
        
        - name: "RESCUE: Log failure to journal"
          shell: 'echo "Pipeline FAILED during processing - rollback complete" | systemd-cat -t srsilo-pipeline -p err'
          become: yes
          become_user: "{{ srsilo_user }}"
          changed_when: false
        
        - name: "RESCUE: Fail with error message"
          fail:
            msg: "Pipeline failed during processing. Rollback completed. Check logs for details."
      
      when: srsilo_has_new_data
  
  post_tasks:
    - name: Log pipeline completion to journal (success)
      shell: echo "Pipeline completed successfully" | systemd-cat -t srsilo-pipeline -p info
      become: yes
      become_user: "{{ srsilo_user }}"
      changed_when: false
      when: srsilo_has_new_data is defined and srsilo_has_new_data
    
    - name: Log pipeline completion to journal (no new data)
      shell: echo "Pipeline completed - no new data" | systemd-cat -t srsilo-pipeline -p info
      become: yes
      become_user: "{{ srsilo_user }}"
      changed_when: false
      when: srsilo_has_new_data is defined and not srsilo_has_new_data
    
    - name: Log pipeline failure to journal
      shell: 'echo "Pipeline FAILED: {{ ansible_failed_result.msg | default(\"Unknown error\") }}" | systemd-cat -t srsilo-pipeline -p err'
      become: yes
      become_user: "{{ srsilo_user }}"
      changed_when: false
      when: ansible_failed_result is defined
      ignore_errors: yes